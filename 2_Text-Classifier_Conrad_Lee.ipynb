{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Text Classifier\n",
    "### Conrad Lee\n",
    "\n",
    "#### Goal:\n",
    "- Train a text classifier that categorizes movie reviews in \"positive\" and \"negative\".\n",
    "\n",
    "#### Steps:\n",
    "- Step 1: Read all the text files from aclImdb/train/pos/ and aclImdb/train/neg/ into a pandas DataFrame called train with two columns: review (the text itself) and sentiment (positive or negative). Do the same thing with aclImdb/test and save it to a different DataFrame. If the dataset is too large for your machine, use a sample. (Hint: Use the glob module) (2 points)\n",
    "- Step 2: Preprocess the text using CountVectorizer or TfidfVectorizer from scikit-learn, adding a preprocessor that removes spurious <br /> tags from the text, or alternatively removing them from the dataframes beforehand (1 point)\n",
    "- Step 3: Apply the Vectorizer to the train data, fit a logistic regression, and compute the accuracy score (1 point)\n",
    "- Step 4: Concatenate the train and test data, and use GridSearchCV to optimize the C hyperparameter of the logistic regression (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "- I have forked and referenced from Jitendra Reddy's excellent Github repository online, primarily with regards to PyPrind (progress indicator feature) and the initial data loading loop. The title of the repository is \"Movie Review Classifier.\"\n",
    "\n",
    "Link: https://github.com/g10draw/movie_review_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pyprind  # Allows to visualize the progress and estimated time until completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glob\n",
    "- glob(pathname, *, recursive=False)\n",
    "- Return a list of paths matching a pathname pattern.\n",
    "- The pattern may contain simple shell-style wildcards a la fnmatch. However, unlike fnmatch, filenames starting with a dot are special cases that are not matched by '*' and '?' patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>12500.0</td>\n",
       "      <td>12500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>12500.0</td>\n",
       "      <td>12500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train     test\n",
       "pos  12500.0  12500.0\n",
       "neg  12500.0  12500.0"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_data = pd.DataFrame()\n",
    "num_data.loc[\"pos\", \"train\"] = len(glob.glob(\"./data/aclImdb/train/pos/*.txt\"))\n",
    "num_data.loc[\"neg\", \"train\"] = len(glob.glob(\"./data/aclImdb/train/neg/*.txt\"))\n",
    "num_data.loc[\"pos\", \"test\"] = len(glob.glob(\"./data/aclImdb/test/pos/*.txt\"))\n",
    "num_data.loc[\"neg\", \"test\"] = len(glob.glob(\"./data/aclImdb/test/neg/*.txt\"))\n",
    "num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 50,000 records in the train and test data sets. Half are positive and half are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling\n",
    "- Let's decide how many records we want to sample from the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPrind (Python Progress Indicator)\n",
    "- ProgPercent(iterations, track_time=True, stream=2, title='', monitor=False, update_interval=None)\n",
    "- Initializes a progress bar object that allows visualization of an iterational computation in the standard output screen.\n",
    "- Iterations = Number of iterations for the iterative computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyPrind\n",
    "pper = pyprind.ProgPercent(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for positive and negative\n",
    "labels = {\"pos\": 1, \"neg\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:02:08 | ETA: 00:00:004\n",
      "Total time elapsed: 00:02:08\n"
     ]
    }
   ],
   "source": [
    "# Load data frame\n",
    "dataframe = pd.DataFrame()\n",
    "\n",
    "for i in (\"train\", \"test\"):\n",
    "    for j in (\"pos\", \"neg\"):\n",
    "        path = \"./data/aclImdb/%s/%s/*.txt\" % (i, j)\n",
    "        for file in glob.glob(path)[0:2500]:\n",
    "            with open(file, \"r\", encoding=\"utf8\") as infile:\n",
    "                text = infile.read()\n",
    "            dataframe = dataframe.append([[text, labels[j]]], ignore_index=True)\n",
    "            pper.update()\n",
    "\n",
    "dataframe.columns = [\"review\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll only use 10,000 records (out of the 50,000 total records)\n",
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a single CSV file.\n",
    "dataframe[0:5000].to_csv(\"./movie_reviews_test_data.csv\", index=False)\n",
    "dataframe[5000:10000].to_csv(\"./movie_reviews_train_data.csv\", index=False)\n",
    "dataframe.to_csv(\"./movie_reviews_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset back into a data variable\n",
    "# Note: This is just in case we use the full dataset, and want to avoid re-importing all the data again\n",
    "data = pd.read_csv(\"movie_reviews_data.csv\")\n",
    "train = pd.read_csv(\"movie_reviews_train_data.csv\")\n",
    "test = pd.read_csv(\"movie_reviews_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer:\n",
    "- Convert a collection of text documents to a matrix of token counts\n",
    "- This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "- If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "- CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)\n",
    "\n",
    "##### preprocessor : callable or None (default)\n",
    "- Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "##### tokenizer : callable or None (default)\n",
    "- Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.\n",
    "\n",
    "##### stop_words : string {‘english’}, list, or None (default)\n",
    "- If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see Using stop words).\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0] to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "##### max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "##### min_df : float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods:\n",
    "\n",
    "##### get_feature_names()\n",
    "- Array mapping from feature integer indices to feature name\n",
    "\n",
    "##### fit_transform(raw_documents[, y])\n",
    "- Learn the vocabulary dictionary and return term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=0.01, max_df=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix that contains the vocabulary dictionary\n",
    "total = vectorizer.fit_transform(data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the dictionary matrix for the train set\n",
    "# Y is the dictionary matrix for the test set\n",
    "X = total[0:5000]\n",
    "Y = total[5000:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dictionary length varies based on the parameters we give CountVectorizer:\n",
    "- CountVectorizer() --> 49,444 words\n",
    "- CountVectorizer(stop_words='english') --> 49,133 words\n",
    "- CountVectorizer(stop_words='english', min_df=0.01, max_df=0.99) --> 1,542 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of words in the dictionary\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '11', '12', '13', '15', '20', '30', '40', '50', '60', '70', '70s', '80', '80s', '90', 'ability', 'able', 'absolute', 'absolutely', 'academy', 'accent', 'accept', 'accident', 'accurate', 'act', 'acted', 'acting', 'action', 'actions', 'actor', 'actors', 'actress', 'actresses', 'acts', 'actual', 'actually', 'adaptation', 'add', 'added', 'addition', 'adds', 'admit', 'adult', 'adults', 'adventure', 'advice', 'affair', 'afraid', 'age', 'agent', 'ago', 'agree', 'ahead', 'air', 'al', 'alive', 'allow', 'allowed', 'amazing', 'america', 'american', 'americans', 'amusing', 'angry', 'animals', 'animation', 'annoying', 'answer', 'anti', 'anybody', 'anymore', 'apart', 'apartment', 'apparent', 'apparently', 'appeal', 'appear', 'appearance', 'appeared', 'appears', 'appreciate', 'approach', 'area', 'aren', 'army', 'art', 'artist', 'artistic', 'arts', 'aside', 'ask', 'asked', 'aspect', 'aspects', 'ass', 'atmosphere', 'attack', 'attempt', 'attempts', 'attention', 'attractive', 'audience', 'audiences', 'available', 'average', 'avoid', 'award', 'away', 'awesome', 'awful', 'baby', 'background', 'bad', 'badly', 'band', 'bar', 'barbara', 'barely', 'based', 'basic', 'basically', 'battle', 'beat', 'beautiful', 'beautifully', 'beauty', 'bed', 'began', 'begin', 'beginning', 'begins', 'believable', 'believe', 'ben', 'best', 'better', 'big', 'biggest', 'billy', 'bit', 'bits', 'bizarre', 'black', 'blame', 'blood', 'bloody', 'blue', 'body', 'book', 'books', 'bored', 'boring', 'born', 'boss', 'bother', 'bought', 'box', 'boy', 'boyfriend', 'boys', 'br', 'brain', 'break', 'breaking', 'brian', 'brief', 'brilliant', 'bring', 'brings', 'british', 'brother', 'brothers', 'brought', 'bruce', 'budget', 'build', 'building', 'bunch', 'business', 'buy', 'cable', 'called', 'calls', 'came', 'camera', 'camp', 'capture', 'captured', 'car', 'care', 'career', 'carry', 'cartoon', 'case', 'cast', 'casting', 'cat', 'catch', 'caught', 'cause', 'central', 'century', 'certain', 'certainly', 'cgi', 'chance', 'change', 'changed', 'changes', 'channel', 'character', 'characters', 'charles', 'charm', 'charming', 'chase', 'cheap', 'check', 'cheesy', 'chemistry', 'child', 'childhood', 'children', 'chinese', 'choice', 'christmas', 'christopher', 'church', 'cinema', 'cinematic', 'cinematography', 'city', 'class', 'classic', 'clear', 'clearly', 'clever', 'cliché', 'clichés', 'climax', 'close', 'clothes', 'club', 'cold', 'collection', 'college', 'color', 'come', 'comedic', 'comedies', 'comedy', 'comes', 'comic', 'coming', 'comment', 'commentary', 'comments', 'common', 'company', 'compare', 'compared', 'comparison', 'compelling', 'complete', 'completely', 'complex', 'computer', 'concept', 'conclusion', 'confused', 'confusing', 'consider', 'considered', 'considering', 'constantly', 'contains', 'content', 'continue', 'control', 'convincing', 'cool', 'cop', 'cops', 'copy', 'costumes', 'couldn', 'count', 'country', 'couple', 'course', 'cover', 'crap', 'crappy', 'crazy', 'create', 'created', 'creating', 'creative', 'creature', 'credit', 'credits', 'creepy', 'crew', 'crime', 'critics', 'cross', 'cult', 'culture', 'current', 'cut', 'cute', 'dad', 'damn', 'dance', 'dancing', 'dangerous', 'dark', 'date', 'daughter', 'david', 'day', 'days', 'dead', 'deal', 'dealing', 'deals', 'death', 'decent', 'decide', 'decided', 'decides', 'deep', 'deeply', 'definitely', 'deliver', 'delivers', 'depth', 'deserve', 'deserves', 'design', 'desire', 'desperate', 'despite', 'details', 'detective', 'developed', 'development', 'dialog', 'dialogue', 'did', 'didn', 'die', 'died', 'dies', 'difference', 'different', 'difficult', 'direct', 'directed', 'directing', 'direction', 'director', 'directors', 'dirty', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'discover', 'discovered', 'disney', 'disturbing', 'doctor', 'documentary', 'does', 'doesn', 'dog', 'doing', 'don', 'door', 'double', 'doubt', 'dr', 'drama', 'dramatic', 'drawn', 'dream', 'dreams', 'drive', 'driving', 'drug', 'drugs', 'dull', 'dumb', 'dvd', 'dying', 'earlier', 'early', 'earth', 'easily', 'easy', 'eat', 'ed', 'edge', 'edited', 'editing', 'effect', 'effective', 'effects', 'effort', 'efforts', 'element', 'elements', 'emotion', 'emotional', 'emotions', 'end', 'ended', 'ending', 'ends', 'energy', 'engaging', 'england', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'entertaining', 'entertainment', 'entire', 'entirely', 'epic', 'episode', 'episodes', 'equally', 'era', 'eric', 'escape', 'especially', 'essentially', 'european', 'event', 'events', 'eventually', 'everybody', 'evil', 'ex', 'exactly', 'example', 'excellent', 'exception', 'exciting', 'excuse', 'expect', 'expectations', 'expected', 'expecting', 'experience', 'explain', 'explained', 'extra', 'extreme', 'extremely', 'eye', 'eyes', 'face', 'faces', 'fact', 'failed', 'fails', 'fair', 'fairly', 'faith', 'fake', 'fall', 'falling', 'falls', 'familiar', 'family', 'famous', 'fan', 'fans', 'fantastic', 'fantasy', 'far', 'fascinating', 'fashion', 'fast', 'fat', 'father', 'favorite', 'favourite', 'fear', 'feature', 'features', 'featuring', 'feel', 'feeling', 'feelings', 'feels', 'fell', 'fellow', 'felt', 'female', 'festival', 'fi', 'fiction', 'field', 'fight', 'fighting', 'figure', 'filled', 'film', 'filmed', 'filming', 'filmmakers', 'films', 'final', 'finally', 'finding', 'finds', 'fine', 'finest', 'finish', 'finished', 'fit', 'flat', 'flaws', 'flick', 'flicks', 'floor', 'flying', 'focus', 'folks', 'follow', 'followed', 'following', 'follows', 'food', 'footage', 'force', 'forced', 'forever', 'forget', 'forgotten', 'form', 'forward', 'frame', 'frank', 'free', 'french', 'fresh', 'friend', 'friends', 'friendship', 'fully', 'fun', 'funniest', 'funny', 'future', 'game', 'games', 'gang', 'gangster', 'garbage', 'gave', 'gay', 'gem', 'general', 'generally', 'genius', 'genre', 'genuinely', 'george', 'german', 'gets', 'getting', 'ghost', 'giant', 'girl', 'girlfriend', 'girls', 'given', 'gives', 'giving', 'glad', 'god', 'goes', 'going', 'gold', 'gone', 'good', 'gore', 'gorgeous', 'got', 'government', 'grace', 'grade', 'great', 'greatest', 'green', 'ground', 'group', 'growing', 'guess', 'gun', 'guy', 'guys', 'hadn', 'hair', 'half', 'hand', 'hands', 'happen', 'happened', 'happening', 'happens', 'happy', 'hard', 'hardly', 'hasn', 'hate', 'hated', 'haven', 'having', 'head', 'heads', 'hear', 'heard', 'heart', 'heavy', 'held', 'hell', 'help', 'helped', 'helps', 'henry', 'hero', 'hey', 'hidden', 'high', 'highly', 'hilarious', 'historical', 'history', 'hit', 'hitchcock', 'hits', 'hold', 'holds', 'holes', 'hollywood', 'home', 'honest', 'honestly', 'hope', 'hopes', 'hoping', 'horrible', 'horror', 'horse', 'hot', 'hour', 'hours', 'house', 'huge', 'human', 'humor', 'humour', 'hurt', 'husband', 'idea', 'ideas', 'ii', 'ill', 'image', 'images', 'imagination', 'imagine', 'imdb', 'immediately', 'impact', 'important', 'impossible', 'impressed', 'impression', 'impressive', 'include', 'includes', 'including', 'incredible', 'incredibly', 'independent', 'indian', 'industry', 'information', 'innocent', 'inside', 'inspired', 'instead', 'intelligence', 'intelligent', 'intended', 'intense', 'interested', 'interesting', 'intriguing', 'introduced', 'involved', 'involving', 'island', 'isn', 'issue', 'issues', 'italian', 'jack', 'james', 'jane', 'japanese', 'jim', 'job', 'joe', 'john', 'joke', 'jokes', 'jones', 'journey', 'joy', 'jr', 'just', 'justice', 'keeping', 'keeps', 'kept', 'kevin', 'key', 'kick', 'kid', 'kids', 'kill', 'killed', 'killer', 'killing', 'kills', 'kind', 'kinda', 'king', 'knew', 'know', 'knowing', 'knowledge', 'known', 'knows', 'la', 'lack', 'lacking', 'lacks', 'ladies', 'lady', 'lame', 'land', 'language', 'large', 'late', 'later', 'laugh', 'laughable', 'laughed', 'laughing', 'laughs', 'law', 'lead', 'leader', 'leading', 'leads', 'learn', 'learned', 'leave', 'leaves', 'leaving', 'led', 'lee', 'left', 'let', 'lets', 'level', 'lies', 'life', 'light', 'lighting', 'like', 'liked', 'likely', 'likes', 'limited', 'line', 'lines', 'list', 'listen', 'literally', 'little', 'live', 'lived', 'lives', 'living', 'll', 'local', 'location', 'locations', 'london', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loose', 'lord', 'lose', 'lost', 'lot', 'lots', 'loud', 'love', 'loved', 'lovely', 'lover', 'loves', 'loving', 'low', 'lucky', 'machine', 'mad', 'magic', 'main', 'mainly', 'major', 'make', 'makers', 'makes', 'making', 'male', 'man', 'manage', 'managed', 'manages', 'manner', 'mark', 'marriage', 'married', 'martin', 'mary', 'master', 'masterpiece', 'match', 'material', 'matter', 'maybe', 'mean', 'meaning', 'means', 'meant', 'mediocre', 'meet', 'meets', 'member', 'members', 'memorable', 'memory', 'men', 'mention', 'mentioned', 'merely', 'mess', 'message', 'met', 'michael', 'mid', 'middle', 'military', 'million', 'mind', 'minor', 'minute', 'minutes', 'miss', 'missed', 'missing', 'mission', 'mistake', 'mix', 'modern', 'mom', 'moment', 'moments', 'money', 'monster', 'months', 'mood', 'moral', 'mother', 'motion', 'mouth', 'moved', 'moves', 'movie', 'movies', 'moving', 'mr', 'ms', 'murder', 'murders', 'music', 'musical', 'mysterious', 'mystery', 'naked', 'named', 'names', 'narrative', 'nasty', 'natural', 'naturally', 'nature', 'near', 'nearly', 'necessary', 'need', 'needed', 'needs', 'negative', 'new', 'news', 'nice', 'nicely', 'night', 'non', 'nonsense', 'normal', 'normally', 'note', 'notice', 'noticed', 'novel', 'nudity', 'number', 'numbers', 'obvious', 'obviously', 'occasionally', 'odd', 'offer', 'offers', 'office', 'oh', 'ok', 'okay', 'old', 'older', 'ones', 'open', 'opening', 'opera', 'opinion', 'opportunity', 'opposite', 'order', 'original', 'originally', 'oscar', 'outside', 'outstanding', 'overall', 'pace', 'paced', 'paid', 'pain', 'painful', 'par', 'parents', 'paris', 'park', 'particular', 'particularly', 'parts', 'party', 'pass', 'passion', 'past', 'pathetic', 'paul', 'pay', 'people', 'perfect', 'perfectly', 'performance', 'performances', 'period', 'person', 'personal', 'personality', 'personally', 'peter', 'phone', 'photography', 'physical', 'pick', 'picked', 'picture', 'pictures', 'piece', 'pieces', 'place', 'places', 'plain', 'plan', 'plane', 'play', 'played', 'player', 'players', 'playing', 'plays', 'pleasure', 'plenty', 'plot', 'plots', 'plus', 'point', 'pointless', 'points', 'police', 'political', 'poor', 'poorly', 'pop', 'popular', 'porn', 'portrayal', 'portrayed', 'positive', 'possible', 'possibly', 'post', 'potential', 'power', 'powerful', 'predictable', 'premise', 'presence', 'present', 'presented', 'pretentious', 'pretty', 'previous', 'prison', 'probably', 'problem', 'problems', 'process', 'produced', 'producer', 'producers', 'production', 'professional', 'project', 'promise', 'protagonist', 'prove', 'proves', 'provide', 'provides', 'psycho', 'psychological', 'public', 'pull', 'pure', 'purpose', 'puts', 'putting', 'quality', 'question', 'questions', 'quick', 'quickly', 'quite', 'radio', 'random', 'rare', 'rarely', 'rate', 'rated', 'rating', 'reaction', 'read', 'reading', 'ready', 'real', 'realism', 'realistic', 'reality', 'realize', 'realized', 'really', 'reason', 'reasons', 'recent', 'recently', 'recommend', 'recommended', 'record', 'red', 'redeeming', 'relate', 'relationship', 'relationships', 'release', 'released', 'religious', 'remains', 'remake', 'remember', 'reminded', 'reminds', 'rent', 'rented', 'respect', 'rest', 'result', 'return', 'returns', 'revenge', 'review', 'reviews', 'rich', 'richard', 'ride', 'ridiculous', 'right', 'rip', 'road', 'robert', 'rock', 'role', 'roles', 'roll', 'romance', 'romantic', 'room', 'run', 'running', 'runs', 'russian', 'sad', 'sadly', 'said', 'sam', 'sat', 'save', 'saving', 'saw', 'say', 'saying', 'says', 'scared', 'scary', 'scene', 'scenery', 'scenes', 'school', 'sci', 'science', 'score', 'scott', 'screen', 'screenplay', 'script', 'sea', 'sean', 'search', 'season', 'second', 'seconds', 'secret', 'seeing', 'seemingly', 'seen', 'sees', 'self', 'sense', 'sent', 'sequel', 'sequence', 'sequences', 'serial', 'series', 'seriously', 'set', 'sets', 'setting', 'seven', 'sex', 'sexual', 'sexy', 'shame', 'share', 'sheer', 'ship', 'shock', 'shocking', 'shoot', 'shooting', 'shop', 'short', 'shot', 'shots', 'shouldn', 'showed', 'showing', 'shown', 'shows', 'sick', 'sight', 'silent', 'silly', 'similar', 'simple', 'simply', 'singer', 'singing', 'single', 'sister', 'sit', 'sitting', 'situation', 'situations', 'slasher', 'sleep', 'slightly', 'slow', 'slowly', 'small', 'smart', 'smile', 'smith', 'soap', 'social', 'society', 'soft', 'soldiers', 'solid', 'somewhat', 'son', 'song', 'songs', 'soon', 'sorry', 'sort', 'soul', 'sound', 'sounds', 'soundtrack', 'south', 'space', 'speak', 'speaking', 'special', 'spectacular', 'spend', 'spent', 'spirit', 'spoiler', 'spoilers', 'spot', 'stage', 'stand', 'standard', 'standards', 'stands', 'star', 'starring', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'states', 'station', 'stay', 'step', 'stephen', 'steve', 'stick', 'stock', 'stop', 'store', 'stories', 'story', 'storyline', 'straight', 'strange', 'street', 'strong', 'struggle', 'stuck', 'student', 'students', 'studio', 'stuff', 'stunning', 'stupid', 'style', 'sub', 'subject', 'subtle', 'success', 'successful', 'sucks', 'suddenly', 'suggest', 'suicide', 'summer', 'super', 'superb', 'support', 'supporting', 'suppose', 'supposed', 'supposedly', 'sure', 'surely', 'surprise', 'surprised', 'surprising', 'surprisingly', 'survive', 'suspect', 'suspense', 'sweet', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking', 'taste', 'team', 'tears', 'teen', 'teenage', 'television', 'tell', 'telling', 'tells', 'tension', 'terms', 'terrible', 'terrific', 'thank', 'thanks', 'thats', 'theater', 'theatre', 'theme', 'themes', 'thing', 'things', 'think', 'thinking', 'thinks', 'thoroughly', 'thought', 'thriller', 'throw', 'thrown', 'time', 'times', 'tired', 'title', 'today', 'told', 'tom', 'tone', 'tony', 'took', 'total', 'totally', 'touch', 'touching', 'tough', 'town', 'track', 'tragedy', 'tragic', 'train', 'trash', 'treat', 'tried', 'tries', 'trip', 'trouble', 'true', 'truly', 'trust', 'truth', 'try', 'trying', 'turn', 'turned', 'turning', 'turns', 'tv', 'twice', 'twist', 'twists', 'type', 'types', 'typical', 'ugly', 'ultimately', 'unbelievable', 'understand', 'understanding', 'unfortunately', 'unique', 'unknown', 'unless', 'unlike', 'unnecessary', 'unusual', 'ups', 'use', 'used', 'uses', 'using', 'usual', 'usually', 'utterly', 'value', 'values', 'van', 'various', 've', 'version', 'victim', 'victims', 'video', 'view', 'viewer', 'viewers', 'viewing', 'villain', 'violence', 'violent', 'visual', 'voice', 'vs', 'wait', 'waiting', 'walk', 'walking', 'wall', 'want', 'wanted', 'wanting', 'wants', 'war', 'warning', 'wasn', 'waste', 'wasted', 'watch', 'watchable', 'watched', 'watching', 'water', 'way', 'ways', 'weak', 'wearing', 'week', 'weird', 'went', 'weren', 'west', 'western', 'whatsoever', 'white', 'wide', 'wife', 'wild', 'william', 'williams', 'willing', 'win', 'window', 'winning', 'wise', 'wish', 'witty', 'woman', 'women', 'won', 'wonder', 'wonderful', 'wonderfully', 'wondering', 'wooden', 'woods', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worse', 'worst', 'worth', 'worthy', 'wouldn', 'wow', 'write', 'writer', 'writers', 'writing', 'written', 'wrong', 'wrote', 'yeah', 'year', 'years', 'yes', 'york', 'young', 'younger', 'zero', 'zombie']\n"
     ]
    }
   ],
   "source": [
    "# Print out the dictionary\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [2 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(total.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression:\n",
    "- LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "##### fit(X, y[, sample_weight])\n",
    "- Fit the model according to the given training data.\n",
    "\n",
    "##### predict(X)\n",
    "- Predict class labels for samples in X.\n",
    "\n",
    "##### predict_proba(X)\n",
    "- Probability estimates.\n",
    "\n",
    "##### score(X, y[, sample_weight])\n",
    "- Returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "##### C : float, default: 1.0\n",
    "- Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model based on the train dictionary vs. train sentiment\n",
    "clf1 = LogisticRegression(solver=\"lbfgs\", max_iter=500).fit(X, train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the results for the test data\n",
    "y_pred1 = clf1.predict(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the predictions for the test set (based on the logistic regression model)\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the actual values for the test set\n",
    "y_true1 = test.sentiment.values\n",
    "y_true1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8186"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score\n",
    "clf1.score(Y.toarray(), test.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "- confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "conf1 = confusion_matrix(y_true1, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the individual TN, FP, FN, and TP\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_true1, y_pred1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true/pos</th>\n",
       "      <th>true/neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred/pos</th>\n",
       "      <td>2022</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred/neg</th>\n",
       "      <td>429</td>\n",
       "      <td>2071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          true/pos  true/neg\n",
       "pred/pos      2022       478\n",
       "pred/neg       429      2071"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's the confusion matrix a pretty dataframe:\n",
    "results = pd.DataFrame({\"true/pos\": [tp1, fp1], \"true/neg\": [fn1, tn1]})\n",
    "results.rename(index={0: \"pred/pos\", 1: \"pred/neg\"}, inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try using LogisticRegressionCV instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.847"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the LinearRegressionCV model\n",
    "clf2 = LogisticRegressionCV(cv=5, solver=\"lbfgs\", max_iter=500).fit(X, train.sentiment)\n",
    "y_pred2 = clf2.predict(Y)\n",
    "y_true2 = test.sentiment.values\n",
    "clf2.score(Y.toarray(), test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true/pos</th>\n",
       "      <th>true/neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred/pos</th>\n",
       "      <td>2133</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred/neg</th>\n",
       "      <td>398</td>\n",
       "      <td>2102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          true/pos  true/neg\n",
       "pred/pos      2133       367\n",
       "pred/neg       398      2102"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix for LinearRegressionCV\n",
    "conf2 = confusion_matrix(y_true2, y_pred2)\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(y_true2, y_pred2).ravel()\n",
    "results2 = pd.DataFrame({\"true/pos\": [tp2, fp2], \"true/neg\": [fn2, tn2]})\n",
    "results2.rename(index={0: \"pred/pos\", 1: \"pred/neg\"}, inplace=True)\n",
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like LogisticRegressionCV performs a little better than the regular version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV\n",
    "- Exhaustive search over specified parameter values for an estimator.\n",
    "- GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "#### Parameters:\n",
    "- GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=None, iid=’warn’, refit=True, cv=’warn’, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise-deprecating’, return_train_score=’warn’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the maximum number of iterations so that 'lbfgs' converges\n",
    "clf_grid = LogisticRegression(solver=\"lbfgs\", max_iter=500).fit(X, train.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try setting C to any integer between 1 and 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': array([1, 2, 3, 4, 5, 6, 7, 8, 9])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\"C\": np.arange(1, 10, 1)}\n",
    "grid = GridSearchCV(clf_grid, parameters, cv=5)\n",
    "grid.fit(total, data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "topC = grid.best_params_[\"C\"]\n",
    "print(topC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8186"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the LinearRegression model with the optimized hyperparameter C:\n",
    "clf3 = LogisticRegression(solver=\"lbfgs\", C=topC, max_iter=500).fit(X, train.sentiment)\n",
    "y_pred3 = clf3.predict(Y)\n",
    "y_true3 = test.sentiment.values\n",
    "clf3.score(Y.toarray(), test.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's also try setting C to a decimal between 0.1 and 1 (in increments of 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\"C\": np.arange(0.1, 1, 0.1)}\n",
    "grid = GridSearchCV(clf_grid, parameters, cv=5)\n",
    "grid.fit(total, data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "topC = grid.best_params_[\"C\"]\n",
    "print(topC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the LinearRegression model with the optimized hyperparameter C:\n",
    "clf3 = LogisticRegression(solver=\"lbfgs\", C=topC, max_iter=500).fit(X, train.sentiment)\n",
    "y_pred3 = clf3.predict(Y)\n",
    "y_true3 = test.sentiment.values\n",
    "clf3.score(Y.toarray(), test.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like C = 0.1 results in the best accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
